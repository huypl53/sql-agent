server:
  host: "0.0.0.0"
  port: 8000
  log_level: "info"
  reload: true
  workers: 1

candidate_generations:
  - 
    prompt_type: "direct_generation"
    query_validation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    generation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    query_fixer_kwargs:
      model: "google_genai:gemini-2.0-flash"
  -
    prompt_type: "cot_generation"
    query_validation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    generation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    query_fixer_kwargs:
      model: "google_genai:gemini-2.0-flash"
  -
    prompt_type: "dac_cot_genration"
    query_validation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    generation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    query_fixer_kwargs:
      model: "google_genai:gemini-2.0-flash"
  -
    prompt_type: "query_plan_generation"
    query_validation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    generation_kwargs:
      model: "google_genai:gemini-2.0-flash"
    query_fixer_kwargs:
      model: "google_genai:gemini-2.0-flash"

merger:
  model: "google_genai:gemini-2.0-flash"

result_enhancement:
  model: "google_genai:gemini-2.0-flash"

schema_linking:
  model: "google_genai:gemini-2.0-flash"
  
# MCP servers
mcp_servers:
  -
    server_name: "mcp-server-chart"
    url: "http://localhost:1122/sse"
    transport: "sse"
    # transport: "streamable_http"
  # - 
  #   server_name: "mcp-server-chart"
  #   command: "mcp-server-chart"
  #   args: ["--transport", "stdio"]
  #   transport: "stdio"
  -
    server_name: "mcp-server-text2sql"
    url: "http://127.0.0.1:8000/sse"
    transport: "sse"
    
orchestrator:
  # model: "mistralai:mistral-small-latest"
  model: "google_genai:gemini-2.0-flash"
  

vector_store:
  collection_name: "gsv"
  persistent_dir: "/mnt/Code/code/AI/agentic-AI/SQL-QA/data/vector/gsv"
  embedding_model: "hiieu/halong_embedding"