import csv
import sys

# Increase CSV field size limit to a reasonable value (100MB)
csv.field_size_limit(100 * 1024 * 1024)

from typing import List, Tuple, Optional, Set, Iterator, Dict, Any, Literal, Union
from pydantic import BaseModel, Field, field_validator
import re
from shared.db import get_db
import click
from enum import Enum, auto
import json
from pathlib import Path
from tqdm import tqdm
import sqlglot
from sql_qa.config import get_app_config

from langgraph.prebuilt import create_react_agent

# from langchain.chat_models import init_chat_model


class MetricType(Enum):
    EXACT_MATCH = auto()
    EXECUTION_MATCH = auto()


app_config = get_app_config()


class DetailedResult(BaseModel):
    question: str = Field(description="User's natural language question")
    generated: str = Field(description="SQL query generated by the model")
    ground_truth: str = Field(description="Ground truth SQL query")
    ground_truth_result: Optional[str] = Field(
        default=None, description="Raw execution result of ground truth query"
    )
    generated_result: Optional[str] = Field(
        default=None, description="Raw execution result of generated SQL query"
    )
    metrics: Dict[str, Any] = Field(description="Evaluation metrics results")
    explanations: Optional[Dict[str, str]] = Field(
        default=None, description="Explanations for failed metrics"
    )



class EvaluationResult(BaseModel):
    metrics: Dict[str, float] = Field(description="Dictionary of metric name to score")
    total_queries: int = Field(description="Total number of queries evaluated")
    detailed_results: List[DetailedResult] = Field(
        description="List of detailed evaluation results"
    )

    def to_dict(self) -> Dict[str, Any]:
        """Convert the evaluation result to a dictionary for JSON serialization."""
        return {
            "summary": {"total_queries": self.total_queries, "metrics": self.metrics},
            "detailed_results": [
                result.model_dump() for result in self.detailed_results
            ],
        }

    def save_to_json(self, filepath: str) -> None:
        """Save the evaluation result to a JSON file with UTF-8 encoding."""
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)


class SQLMetrics:
    def __init__(self, metrics: Set[MetricType] = None):
        """Initialize the metrics evaluator.

        Args:
            metrics: Set of metrics to evaluate. If None, evaluates all metrics.
        """
        self.db = get_db()
        self.metrics = metrics or set(MetricType)

    def _normalize_sql(self, sql: str) -> str:
        """Normalize SQL query for comparison by:
        1. Converting to lowercase
        2. Removing extra whitespace
        3. Standardizing quotes
        4. Removing semicolons

        Args:
            sql: SQL query string

        Returns:
            Normalized SQL query string
        """
        try:
            return sqlglot.transpile(
                sql, write=app_config.database.dialect.lower(), pretty=True
            )[0]
        except Exception:
            return ""

    def _execute_query(self, sql: str) -> Optional[List[Tuple]]:
        """Execute a SQL query and return the results.

        Args:
            sql: SQL query string

        Returns:
            Query results as a list of tuples, or None if execution fails
        """
        try:
            return self.db.run(sql)
        except Exception as e:
            print(f"Error executing query: {e}")
            return None

    def evaluate_queries(
        self, predicted_queries: List[str], ground_truth_queries: List[str]
    ) -> EvaluationResult:
        """Evaluate a list of predicted queries against ground truth queries.

        Args:
            predicted_queries: List of predicted SQL queries
            ground_truth_queries: List of ground truth SQL queries

        Returns:
            EvaluationResult containing requested metrics
        """
        # if len(predicted_queries) != len(ground_truth_queries):
        #     raise ValueError(
        #         "Number of predicted queries must match number of ground truth queries"
        #     )

        detailed_results = []
        metric_scores = {metric: 0 for metric in self.metrics}
        # total_queries = len(predicted_queries)
        total_queries = 0

        for pred, truth in zip(predicted_queries, ground_truth_queries):
            total_queries += 1
            query_results = {}

            # Evaluate exact match if requested
            if MetricType.EXACT_MATCH in self.metrics:
                norm_pred = self._normalize_sql(pred)
                norm_truth = self._normalize_sql(truth)
                is_exact_match = norm_pred == norm_truth
                query_results["exact_match"] = is_exact_match
                if is_exact_match:
                    metric_scores[MetricType.EXACT_MATCH] += 1

            # Evaluate execution match if requested
            if MetricType.EXECUTION_MATCH in self.metrics:
                pred_results = self._execute_query(pred)
                truth_results = self._execute_query(truth)
                is_execution_match = (
                    pred_results is not None
                    and truth_results is not None
                    and pred_results == truth_results
                )
                query_results["execution_match"] = is_execution_match
                if is_execution_match:
                    metric_scores[MetricType.EXECUTION_MATCH] += 1

            detailed_results.append(
                DetailedResult(
                    question=pred,
                    generated=pred,
                    ground_truth=truth,
                    ground_truth_result=truth_results[0][0] if truth_results else None,
                    generated_result=pred_results[0][0] if pred_results else None,
                    metrics=query_results,
                )
            )

        # Calculate final scores
        final_scores = {
            metric.name.lower(): score / total_queries if total_queries > 0 else 0
            for metric, score in metric_scores.items()
        }

        return EvaluationResult(
            metrics=final_scores,
            total_queries=total_queries,
            detailed_results=detailed_results,
        )


def read_sql_pairs_from_csv(csv_file: str) -> Iterator[Dict[str, Any]]:
    """Read SQL pairs from CSV file row by row.

    Expected CSV columns:
    - question: User's natural language question
    - ground_truth_sql: Ground truth SQL query
    - level: Difficulty level (easy/hard)
    - ground_truth_result: Raw execution result of ground truth query
    - error: Error message from ground truth query execution (if any)
    - generated_sql_query: SQL query generated by LLM
    - generated_query_result: Refined result from generated SQL query execution
    - generated_sql_error: Error message from generated SQL query execution (if any)
    - generated_raw_result: Raw execution result of generated SQL query

    Args:
        csv_file: Path to the CSV file

    Yields:
        Dictionary containing the same fields as the CSV
    """
    with open(csv_file, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            yield {
                "question": row["question"],
                "ground_truth_sql": row["ground_truth_sql"],
                "level": row["level"],
                "ground_truth_result": row["ground_truth_result"],
                "error": row["error"],
                "generated_sql_query": row["generated_sql_query"],
                "generated_query_result": row["generated_query_result"],
                "generated_sql_error": row["generated_sql_error"],
                "generated_raw_result": row["generated_raw_result"],
            }


@click.group()
def cli():
    """SQL evaluation metrics CLI."""
    pass


@cli.command()
@click.option(
    "--predicted-file", required=True, help="File containing predicted SQL queries"
)
@click.option(
    "--ground-truth-file",
    required=True,
    help="File containing ground truth SQL queries",
)
@click.option(
    "--metrics",
    multiple=True,
    type=click.Choice(["exact_match", "execution_match"]),
    default=["exact_match", "execution_match"],
    help="Metrics to evaluate",
)
@click.option(
    "--output-file",
    required=True,
    help="File to write evaluation results (JSON format)",
)
def evaluate_files(
    predicted_file: str, ground_truth_file: str, metrics: List[str], output_file: str
):
    """Evaluate SQL queries from separate files using specified metrics and write results to JSON file."""
    # Convert metric names to MetricType enum
    metric_types = {MetricType[m.upper()] for m in metrics}

    # Initialize evaluator
    evaluator = SQLMetrics(metric_types)

    # Read queries from files
    with open(predicted_file, "r", encoding="utf-8") as f:
        predicted_queries = [line.strip() for line in f if line.strip()]

    with open(ground_truth_file, "r", encoding="utf-8") as f:
        ground_truth_queries = [line.strip() for line in f if line.strip()]

    # Evaluate queries with progress bar
    detailed_results = []
    metric_scores = {metric: 0 for metric in metric_types}

    for pred, truth in tqdm(
        zip(predicted_queries, ground_truth_queries),
        desc="Evaluating queries",
        unit="query",
    ):
        query_results = {}

        # Evaluate exact match if requested
        if MetricType.EXACT_MATCH in metric_types:
            norm_pred = evaluator._normalize_sql(pred)
            norm_truth = evaluator._normalize_sql(truth)
            is_exact_match = norm_pred == norm_truth
            query_results["exact_match"] = is_exact_match
            if is_exact_match:
                metric_scores[MetricType.EXACT_MATCH] += 1

        # Evaluate execution match if requested
        if MetricType.EXECUTION_MATCH in metric_types:
            pred_results = evaluator._execute_query(pred)
            truth_results = evaluator._execute_query(truth)
            is_execution_match = (
                pred_results is not None
                and truth_results is not None
                and pred_results == truth_results
            )
            query_results["execution_match"] = is_execution_match
            if is_execution_match:
                metric_scores[MetricType.EXECUTION_MATCH] += 1

        detailed_results.append(
            DetailedResult(
                question=pred,
                generated=pred,
                ground_truth=truth,
                ground_truth_result=truth_results[0][0] if truth_results else None,
                generated_result=pred_results[0][0] if pred_results else None,
                metrics=query_results,
            )
        )

    # Calculate final scores
    total_queries = len(predicted_queries)
    final_scores = {
        metric.name.lower(): score / total_queries if total_queries > 0 else 0
        for metric, score in metric_scores.items()
    }

    # Create result object
    result = EvaluationResult(
        metrics=final_scores,
        total_queries=total_queries,
        detailed_results=detailed_results,
    )

    # Write results to JSON file
    result.save_to_json(output_file)
    click.echo(f"Evaluation results written to {output_file}")


@cli.command()
@click.option(
    "--input-file", required=True, help="CSV file containing SQL pairs and results"
)
@click.option(
    "--metrics",
    multiple=True,
    type=click.Choice(["exact_match", "execution_match"]),
    default=["exact_match", "execution_match"],
    help="Metrics to evaluate",
)
@click.option(
    "--output-file",
    required=True,
    help="File to write evaluation results (JSON format)",
)
@click.option(
    "--skipped-file",
    default="skipped_queries.json",
    help="File to write details of skipped queries (JSON format)",
)
@click.option(
    "--max-result-length",
    default=1000,
    help="Maximum length of result strings before skipping",
    type=int,
)
def evaluate_csv(
    input_file: str,
    metrics: List[str],
    output_file: str,
    skipped_file: str,
    max_result_length: int,
):
    """Evaluate SQL queries from CSV file using specified metrics and write results to JSON file."""
    # Convert metric names to MetricType enum
    metric_types = {MetricType[m.upper()] for m in metrics}

    # Initialize evaluator
    evaluator = SQLMetrics(metric_types)

    # Filter SQL pairs based on length constraints
    from sql_qa.metrics.filter import filter_csv_file

    filtered_pairs, num_skipped = filter_csv_file(
        input_file, max_result_length, skipped_file
    )

    # Evaluate queries
    result = evaluator.evaluate_queries(filtered_pairs)

    # Write results to JSON file
    result.save_to_json(output_file)
    click.echo(f"Evaluation results written to {output_file}")
    click.echo(f"Total queries evaluated: {result.total_queries}")
    click.echo(f"Total queries skipped: {num_skipped}")


class MatchResult(BaseModel):
    """Result of a single match evaluation."""

    score: int = Field(
        description="Binary score (0 or 1) indicating match status", ge=0, le=1
    )
    explanation: str = Field(
        description="Explanation if score is 0, empty string if score is 1", default=""
    )


class BinaryEvaluationResponse(BaseModel):
    """Response format for binary SQL evaluation."""

    sql_match: MatchResult = Field(description="SQL match evaluation result")
    result_match: MatchResult = Field(description="Result match evaluation result")


JUDGE_DETAILS_PROMPT = """
You are an expert SQL data analyst and quality assurance specialist. Your task is to meticulously evaluate a predicted SQL query and its execution result against a natural language question (NLQ) and a ground truth SQL query with its result. Since the database schema is not explicitly provided, you will need to infer schema validity from the successful execution of the queries and the context of the NLQ.

**INPUTS:**

1.  **Natural Language Question (NLQ):**
    ```
    {NLQ}
    ```

2.  **Predicted SQL Query:**
    ```sql
    {PREDICTED_SQL}
    ```

3.  **Predicted SQL's Execution Result:**
    (Format: e.g., Markdown table, JSON array of objects, or CSV. An empty result is still a valid result.)
    ```
    {PREDICTED_RESULT}
    ```

4.  **Ground Truth SQL Query:**
    ```sql
    {GROUND_TRUTH_SQL}
    ```

5.  **Ground Truth SQL's Execution Result:**
    (Format: e.g., Markdown table, JSON array of objects, or CSV. An empty result is still a valid result.)
    ```
    {GROUND_TRUTH_RESULT}
    ```

**EVALUATION CRITERIA & OUTPUT FORMAT:**

For each criterion below, provide a binary score (0 or 1) and a brief justification if the score is 0. Assume that if a `Predicted SQL's Execution Result` is provided, the `Predicted SQL Query` was syntactically valid enough to execute.

**CRITERIA:**

**A. SQL Query Evaluation (Comparing Predicted SQL to NLQ):**

1.  **A1. Logical Soundness of Predicted SQL for NLQ:**
    *   Description: Does the `Predicted SQL Query` appear to use table and column names that are plausible given the `NLQ`? Does the query structure (joins, conditions, aggregations) logically attempt to address the core components and constraints mentioned in the `NLQ`? (Focus on the logic, assuming tables/columns used are valid since it executed).
    *   Score (0/1):
    *   Justification (if 0):

2.  **A2. NLQ Component Coverage by SQL Logic:**
    *   Description: Does the *logic* of the `Predicted SQL Query` (SELECT columns, FROM tables, WHERE conditions, GROUP BY, ORDER BY, aggregations, etc.) attempt to address all key components, entities, relationships, and constraints explicitly or implicitly mentioned in the `NLQ`?
    *   Score (0/1):
    *   Justification (if 0):

**B. Execution Result Evaluation (Comparing Predicted Result to NLQ & Ground Truth Result):**

3.  **B1. Result Set Equivalence (Exact Match):**
    *   Description: Does the `Predicted SQL's Execution Result` exactly match the `Ground Truth SQL's Execution Result`? Consider columns present, row content, and row count. Minor differences in row order are acceptable if no `ORDER BY` is specified in the NLQ or Ground Truth SQL. Column order differences in the output are acceptable if the same data is represented.
    *   Score (0/1):
    *   Justification (if 0):

4.  **B2. Result Accurately Answers NLQ (Semantic Match of Data):**
    *   Description: Irrespective of an exact match to the ground truth result, does the data in `Predicted SQL's Execution Result` accurately and completely answer the `NLQ`? (e.g., if NLQ asks for "names of students older than 20", does the result contain only such names and all of them?)
    *   Score (0/1):
    *   Justification (if 0):

5.  **B3. Absence of Hallucinated or Extraneous Data in Result:**
    *   Description: Does the `Predicted SQL's Execution Result` avoid including data not explicitly requested by the `NLQ` or data that seems irrelevant to the question?
    *   Score (0/1):
    *   Justification (if 0):

**C. Overall Assessment:**

6.  **C1. Overall Functional Correctness:**
    *   Description: Considering both the apparent SQL logic and its execution result, does the `Predicted SQL Query` functionally achieve the goal stated in the `NLQ`? This is a holistic judgment.
    *   Score (0/1):
    *   Justification (if 0):

**OUTPUT STRUCTURE:**

Please provide your evaluation in the following format:

```json
{
  "A1_Logical_Soundness_of_SQL": {
    "score": <0_or_1>,
    "justification": "<text_if_score_is_0_else_empty_string>"
  },
  "A2_NLQ_Component_Coverage_by_SQL": {
    "score": <0_or_1>,
    "justification": "<text_if_score_is_0_else_empty_string>"
  },
  "B1_Result_Set_Equivalence": {
    "score": <0_or_1>,
    "justification": "<text_if_score_is_0_else_empty_string>"
  },
  "B2_Result_Accurately_Answers_NLQ": {
    "score": <0_or_1>,
    "justification": "<text_if_score_is_0_else_empty_string>"
  },
  "B3_Absence_of_Extraneous_Data": {
    "score": <0_or_1>,
    "justification": "<text_if_score_is_0_else_empty_string>"
  },
  "C1_Overall_Functional_Correctness": {
    "score": <0_or_1>,
    "justification": "<text_if_score_is_0_else_empty_string>"
  }
}
```
"""

JUDGE_BINARY_PROMPT = """You are an expert SQL data analyst and quality assurance specialist. Your task is to evaluate a predicted SQL query and its execution result against a ground truth SQL query and its result.

**INPUTS:**

1. Natural Language Question (NLQ):
```
{NLQ}
```

2. Predicted SQL Query:
```sql
{PREDICTED_SQL}
```

3. Predicted SQL's Execution Result:
```
{PREDICTED_RESULT}
```

4. Ground Truth SQL Query:
```sql
{GROUND_TRUTH_SQL}
```

5. Ground Truth SQL's Execution Result:
```
{GROUND_TRUTH_RESULT}
```

**EVALUATION CRITERIA:**

1. SQL Match (0/1): 
   - Score 1 if the predicted SQL query is semantically equivalent to the ground truth SQL query
   - Consider table/column usage, filtering conditions, aggregations, and sorting
   - Minor differences in syntax or formatting are acceptable
   - Score 0 if the queries are semantically different

2. Result Match (0/1):
   - Score 1 if the predicted result exactly matches the ground truth result
   - Consider both data content and structure
   - Minor differences in formatting are acceptable
   - Score 0 if the results are different

**OUTPUT FORMAT:**
You must return a JSON object with exactly this structure:
{{
  "sql_match": {{
    "score": 0 or 1,
    "explanation": "Brief explanation if score is 0, empty string if score is 1"
  }},
  "result_match": {{
    "score": 0 or 1,
    "explanation": "Brief explanation if score is 0, empty string if score is 1"
  }}
}}

Example response:
{{
  "sql_match": {{
    "score": 1,
    "explanation": ""
  }},
  "result_match": {{
    "score": 0,
    "explanation": "Predicted result has different column order"
  }}
}}"""


class LLMMetrics:
    def __init__(self, model: str = None, prompt: str = JUDGE_DETAILS_PROMPT):
        """Initialize the LLM-based metrics evaluator.

        Args:
            model: The LLM model to use for evaluation. If None, uses the default model from config.
            prompt: The prompt template to use for evaluation. Defaults to detailed evaluation.
        """
        self.tools = []
        self.prompt = prompt

        # Set up response format for binary evaluation
        response_format = None
        if prompt == JUDGE_BINARY_PROMPT:
            response_format = BinaryEvaluationResponse

        self.agent_executor = create_react_agent(
            model, self.tools, prompt=prompt, response_format=response_format
        )

    def evaluate_query_pair(
        self,
        nlq: str,
        generated: str,
        ground_truth: str,
        generated_result: str,
        ground_truth_result: str,
    ) -> Dict[str, Any]:
        """Evaluate a single pair of SQL queries and their results using LLM.

        Args:
            nlq: The natural language question
            generated: The generated SQL query
            ground_truth: The ground truth SQL query
            generated_result: The execution result of the generated query
            ground_truth_result: The execution result of the ground truth query

        Returns:
            Dictionary containing evaluation results
        """
        try:
            prompt = self.prompt.format(
                NLQ=nlq,
                PREDICTED_SQL=generated,
                PREDICTED_RESULT=generated_result,
                GROUND_TRUTH_SQL=ground_truth,
                GROUND_TRUTH_RESULT=ground_truth_result,
            )

            try:
                response = self.agent_executor.invoke(
                    {"messages": [{"role": "user", "content": prompt}]}
                )
            except Exception as e:
                print(f"LLM API error: {str(e)}")
                return self._get_error_response("LLM API error")

            try:
                if self.prompt == JUDGE_BINARY_PROMPT:
                    # Parse response into BinaryEvaluationResponse
                    # evaluation = BinaryEvaluationResponse.model_validate_json(
                    #     response["messages"][-1].content
                    # ).model_dump()
                    evaluation: BinaryEvaluationResponse = response[
                        "structured_response"
                    ]
                    evaluation = evaluation.model_dump()
                    # print(f"Evaluation: {evaluation}")
                else:
                    evaluation = json.loads(response["messages"][-1].content)
            except (json.JSONDecodeError, KeyError, IndexError, ValueError) as e:
                print(f"Failed to parse LLM response: {str(e)}")
                print(f"Raw response: {response}")
                return self._get_error_response("Failed to parse LLM response")

            # Validate response structure for detailed evaluation
            if self.prompt != JUDGE_BINARY_PROMPT:
                required_keys = {
                    "A1_Logical_Soundness_of_SQL",
                    "A2_NLQ_Component_Coverage_by_SQL",
                    "B1_Result_Set_Equivalence",
                    "B2_Result_Accurately_Answers_NLQ",
                    "B3_Absence_of_Extraneous_Data",
                    "C1_Overall_Functional_Correctness",
                }
                if not all(key in evaluation for key in required_keys):
                    print(
                        f"Missing required keys in response. Expected {required_keys}, got {set(evaluation.keys())}"
                    )
                    return self._get_error_response("Invalid response structure")

            return evaluation

        except Exception as e:
            print(f"Unexpected error in evaluate_query_pair: {str(e)}")
            return self._get_error_response("Unexpected error")

    def _get_error_response(self, error_message: str) -> Dict[str, Any]:
        """Generate a standardized error response based on the prompt type.

        Args:
            error_message: Description of the error that occurred

        Returns:
            Dictionary containing error response in the appropriate format
        """
        if self.prompt == JUDGE_BINARY_PROMPT:
            return {
                "sql_match": {
                    "score": 0,
                    "explanation": error_message,
                },
                "result_match": {
                    "score": 0,
                    "explanation": error_message,
                },
            }
        else:
            return {
                "A1_Logical_Soundness_of_SQL": {
                    "score": 0,
                    "justification": error_message,
                },
                "A2_NLQ_Component_Coverage_by_SQL": {
                    "score": 0,
                    "justification": error_message,
                },
                "B1_Result_Set_Equivalence": {
                    "score": 0,
                    "justification": error_message,
                },
                "B2_Result_Accurately_Answers_NLQ": {
                    "score": 0,
                    "justification": error_message,
                },
                "B3_Absence_of_Extraneous_Data": {
                    "score": 0,
                    "justification": error_message,
                },
                "C1_Overall_Functional_Correctness": {
                    "score": 0,
                    "justification": error_message,
                },
            }

    def evaluate_queries(self, sql_pairs: Iterator[Dict[str, Any]]) -> EvaluationResult:
        """Evaluate a list of predicted queries against ground truth queries using LLM.

        Args:
            sql_pairs: Iterator of dictionaries containing SQL pairs and their results

        Returns:
            EvaluationResult containing requested metrics
        """
        detailed_results = []
        # Initialize metrics based on prompt type
        if self.prompt == JUDGE_BINARY_PROMPT:
            metric_scores = {
                "sql_match": 0,
                "result_match": 0,
            }
        else:
            metric_scores = {
                "A1_Logical_Soundness_of_SQL": 0,
                "A2_NLQ_Component_Coverage_by_SQL": 0,
                "B1_Result_Set_Equivalence": 0,
                "B2_Result_Accurately_Answers_NLQ": 0,
                "B3_Absence_of_Extraneous_Data": 0,
                "C1_Overall_Functional_Correctness": 0,
            }
        total_queries = 0

        for pair in tqdm(sql_pairs, desc="Evaluating queries with LLM"):
            total_queries += 1
            evaluation = self.evaluate_query_pair(
                pair["question"],
                pair["generated_sql_query"],
                pair["ground_truth_sql"],
                pair["generated_raw_result"],
                pair["ground_truth_result"],
            )

            # Update scores
            if self.prompt == JUDGE_BINARY_PROMPT:
                # For binary evaluation, we get a BinaryEvaluationResponse
                metric_scores["sql_match"] += evaluation["sql_match"]["score"]
                metric_scores["result_match"] += evaluation["result_match"]["score"]

                metrics = {
                    "sql_match": bool(evaluation["sql_match"]["score"]),
                    "result_match": bool(evaluation["result_match"]["score"]),
                }
                explanations = {
                    "sql_match": evaluation["sql_match"]["explanation"],
                    "result_match": evaluation["result_match"]["explanation"],
                }
            else:
                # For detailed evaluation, we get a dictionary of criteria
                for criterion, result in evaluation.items():
                    metric_scores[criterion] += result["score"]
                metrics = evaluation
                explanations = None

            detailed_results.append(
                DetailedResult(
                    question=pair["question"],
                    generated=pair["generated_sql_query"],
                    ground_truth=pair["ground_truth_sql"],
                    ground_truth_result=pair["ground_truth_result"],
                    generated_result=pair["generated_raw_result"],
                    metrics=metrics,
                    explanations=explanations,
                )
            )

        # Calculate final scores
        final_scores = {
            metric: score / total_queries if total_queries > 0 else 0
            for metric, score in metric_scores.items()
        }

        return EvaluationResult(
            metrics=final_scores,
            total_queries=total_queries,
            detailed_results=detailed_results,
        )


@cli.command()
@click.option(
    "--input-file", required=True, help="CSV file containing SQL pairs and results"
)
@click.option(
    "--output-file",
    required=True,
    help="File to write evaluation results (JSON format)",
)
@click.option(
    "--model",
    default=None,
    help="LLM model to use for evaluation. If not specified, uses the default model from config.",
)
@click.option(
    "--skipped-file",
    default="skipped_queries.csv",
    help="File to write details of skipped queries (CSV format)",
)
@click.option(
    "--max-result-length",
    default=1000,
    help="Maximum length of result strings before skipping",
    type=int,
)
def evaluate_llm(
    input_file: str,
    output_file: str,
    model: str = None,
    skipped_file: str = "skipped_queries.csv",
    max_result_length: int = 1000,
):
    """Evaluate SQL queries using LLM-as-judge with detailed criteria and write results to JSON file."""
    # Initialize LLM evaluator with detailed prompt
    evaluator = LLMMetrics(model=model, prompt=JUDGE_DETAILS_PROMPT)

    # Filter SQL pairs based on length constraints
    from sql_qa.metrics.filter import filter_csv_file

    filtered_pairs, num_skipped = filter_csv_file(
        input_file, max_result_length, skipped_file
    )

    # Evaluate queries using the filtered generator
    result = evaluator.evaluate_queries(filtered_pairs)

    # Write results to JSON file
    result.save_to_json(output_file)
    click.echo(f"Detailed LLM evaluation results written to {output_file}")
    click.echo(f"Total queries evaluated: {result.total_queries}")
    click.echo(f"Total queries skipped: {num_skipped}")
    for metric, score in result.metrics.items():
        click.echo(f"{metric}: {score:.2%}")


@cli.command()
@click.option(
    "--input-file", required=True, help="CSV file containing SQL pairs and results"
)
@click.option(
    "--output-file",
    required=True,
    help="File to write evaluation results (JSON format)",
)
@click.option(
    "--model",
    default=None,
    help="LLM model to use for evaluation. If not specified, uses the default model from config.",
)
@click.option(
    "--skipped-file",
    default="skipped_queries.csv",
    help="File to write details of skipped queries (CSV format)",
)
@click.option(
    "--max-result-length",
    default=1000,
    help="Maximum length of result strings before skipping",
    type=int,
)
def evaluate_llm_binary(
    input_file: str,
    output_file: str,
    model: str = None,
    skipped_file: str = "skipped_queries.csv",
    max_result_length: int = 1000,
):
    """Evaluate SQL queries using LLM-as-judge with binary scores (0/1) for SQL match and result match."""
    # Initialize LLM evaluator with binary prompt
    evaluator = LLMMetrics(model=model, prompt=JUDGE_BINARY_PROMPT)

    # Filter SQL pairs based on length constraints
    from sql_qa.metrics.filter import filter_csv_file

    filtered_pairs, num_skipped = filter_csv_file(
        input_file, max_result_length, skipped_file
    )

    # Evaluate queries using the filtered generator
    result = evaluator.evaluate_queries(filtered_pairs)

    # Write results to JSON file
    result.save_to_json(output_file)
    click.echo(f"Binary LLM evaluation results written to {output_file}")
    click.echo(f"Total queries evaluated: {result.total_queries}")
    click.echo(f"Total queries skipped: {num_skipped}")
    click.echo(f"SQL Match Accuracy: {result.metrics['sql_match']:.2%}")
    click.echo(f"Result Match Accuracy: {result.metrics['result_match']:.2%}")


if __name__ == "__main__":
    cli()
